{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# In this  Notebook, I have Created Image Caption Generator using ResNet and Transformer Decoder Model. It can be divided into 2 steps :\n## **Step 1** : Create features for Images Using Resnet\n## **Step 2** : Train Transformer Decoder Model which predicts next word given a sequence of tokens and Image Features from Step1\n## Please do UpVote this notebook if you liked the content","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import Counter \nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport math\nimport torch.nn.functional as F\nimport pickle\nimport gc\nimport random\npd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:39.138303Z","iopub.execute_input":"2022-02-26T18:47:39.138693Z","iopub.status.idle":"2022-02-26T18:47:39.145916Z","shell.execute_reply.started":"2022-02-26T18:47:39.138640Z","shell.execute_reply":"2022-02-26T18:47:39.145000Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Read Data.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/flickr8k/captions.txt\", sep=',')\nprint(len(df))\ndisplay(df.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:39.147508Z","iopub.execute_input":"2022-02-26T18:47:39.148045Z","iopub.status.idle":"2022-02-26T18:47:39.210296Z","shell.execute_reply.started":"2022-02-26T18:47:39.147966Z","shell.execute_reply":"2022-02-26T18:47:39.209360Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing -> Remove Single Character and non alpha Words. Add <Start>, <end> and <pad> tokens. <pad> token is appended such that length in max_seq_len (maximum length across all captions which is 33 in our case)  ","metadata":{}},{"cell_type":"code","source":"def remove_single_char_word(word_list):\n    lst = []\n    for word in word_list:\n        if len(word)>1:\n            lst.append(word)\n\n    return lst","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:39.212176Z","iopub.execute_input":"2022-02-26T18:47:39.212513Z","iopub.status.idle":"2022-02-26T18:47:39.216998Z","shell.execute_reply.started":"2022-02-26T18:47:39.212476Z","shell.execute_reply":"2022-02-26T18:47:39.216081Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df['cleaned_caption'] = df['caption'].apply(lambda caption : ['<start>'] + [word.lower() if word.isalpha() else '' for word in caption.split(\" \")] + ['<end>'])\ndf['cleaned_caption']  = df['cleaned_caption'].apply(lambda x : remove_single_char_word(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:39.218731Z","iopub.execute_input":"2022-02-26T18:47:39.219313Z","iopub.status.idle":"2022-02-26T18:47:39.603945Z","shell.execute_reply.started":"2022-02-26T18:47:39.219278Z","shell.execute_reply":"2022-02-26T18:47:39.603028Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df['seq_len'] = df['cleaned_caption'].apply(lambda x : len(x))\nmax_seq_len = df['seq_len'].max()\nprint(max_seq_len)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:39.605330Z","iopub.execute_input":"2022-02-26T18:47:39.605666Z","iopub.status.idle":"2022-02-26T18:47:39.634859Z","shell.execute_reply.started":"2022-02-26T18:47:39.605632Z","shell.execute_reply":"2022-02-26T18:47:39.633920Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df.drop(['seq_len'], axis = 1, inplace = True)\ndf['cleaned_caption'] = df['cleaned_caption'].apply(lambda caption : caption + ['<pad>']*(max_seq_len-len(caption)) )","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:39.636162Z","iopub.execute_input":"2022-02-26T18:47:39.636696Z","iopub.status.idle":"2022-02-26T18:47:39.729211Z","shell.execute_reply.started":"2022-02-26T18:47:39.636657Z","shell.execute_reply":"2022-02-26T18:47:39.728314Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"display(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:39.730539Z","iopub.execute_input":"2022-02-26T18:47:39.730883Z","iopub.status.idle":"2022-02-26T18:47:39.744027Z","shell.execute_reply.started":"2022-02-26T18:47:39.730846Z","shell.execute_reply":"2022-02-26T18:47:39.742862Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Create Vocab and mapping of token to ID","metadata":{}},{"cell_type":"code","source":"word_list = df['cleaned_caption'].apply(lambda x : \" \".join(x)).str.cat(sep = ' ').split(' ')\nword_dict = Counter(word_list)\nword_dict =  sorted(word_dict, key=word_dict.get, reverse=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:39.747513Z","iopub.execute_input":"2022-02-26T18:47:39.747787Z","iopub.status.idle":"2022-02-26T18:47:40.068857Z","shell.execute_reply.started":"2022-02-26T18:47:39.747762Z","shell.execute_reply":"2022-02-26T18:47:40.068005Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(len(word_dict))\nprint(word_dict[:5])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.071426Z","iopub.execute_input":"2022-02-26T18:47:40.071680Z","iopub.status.idle":"2022-02-26T18:47:40.077038Z","shell.execute_reply.started":"2022-02-26T18:47:40.071655Z","shell.execute_reply":"2022-02-26T18:47:40.076029Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Vocab size is 8360","metadata":{}},{"cell_type":"code","source":"vocab_size = len(word_dict)\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.078429Z","iopub.execute_input":"2022-02-26T18:47:40.078992Z","iopub.status.idle":"2022-02-26T18:47:40.087940Z","shell.execute_reply.started":"2022-02-26T18:47:40.078956Z","shell.execute_reply":"2022-02-26T18:47:40.086966Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"index_to_word = {index: word for index, word in enumerate(word_dict)}\nword_to_index = {word: index for index, word in enumerate(word_dict)}\nprint(len(index_to_word), len(word_to_index))","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.089138Z","iopub.execute_input":"2022-02-26T18:47:40.089670Z","iopub.status.idle":"2022-02-26T18:47:40.100047Z","shell.execute_reply.started":"2022-02-26T18:47:40.089633Z","shell.execute_reply":"2022-02-26T18:47:40.099047Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Covert sequence of tokens to IDs","metadata":{}},{"cell_type":"code","source":"df['text_seq']  = df['cleaned_caption'].apply(lambda caption : [word_to_index[word] for word in caption] )","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.101533Z","iopub.execute_input":"2022-02-26T18:47:40.102179Z","iopub.status.idle":"2022-02-26T18:47:40.449264Z","shell.execute_reply.started":"2022-02-26T18:47:40.102141Z","shell.execute_reply":"2022-02-26T18:47:40.447807Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"display(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.450943Z","iopub.execute_input":"2022-02-26T18:47:40.456011Z","iopub.status.idle":"2022-02-26T18:47:40.484214Z","shell.execute_reply.started":"2022-02-26T18:47:40.455940Z","shell.execute_reply":"2022-02-26T18:47:40.482037Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split In Train and validation data. Same Image should not be present in both training and validation data ","metadata":{}},{"cell_type":"code","source":"df = df.sort_values(by = 'image')\ntrain = df.iloc[:int(0.9*len(df))]\nvalid = df.iloc[int(0.9*len(df)):]","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.488292Z","iopub.execute_input":"2022-02-26T18:47:40.488677Z","iopub.status.idle":"2022-02-26T18:47:40.567170Z","shell.execute_reply.started":"2022-02-26T18:47:40.488636Z","shell.execute_reply":"2022-02-26T18:47:40.564121Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(len(train), train['image'].nunique())\nprint(len(valid), valid['image'].nunique())","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.571443Z","iopub.execute_input":"2022-02-26T18:47:40.571803Z","iopub.status.idle":"2022-02-26T18:47:40.588008Z","shell.execute_reply.started":"2022-02-26T18:47:40.571765Z","shell.execute_reply":"2022-02-26T18:47:40.587188Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract features from Images Using Resnet","metadata":{}},{"cell_type":"code","source":"train_samples = len(train)\nprint(train_samples)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.589320Z","iopub.execute_input":"2022-02-26T18:47:40.589860Z","iopub.status.idle":"2022-02-26T18:47:40.595525Z","shell.execute_reply.started":"2022-02-26T18:47:40.589825Z","shell.execute_reply":"2022-02-26T18:47:40.594460Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"unq_train_imgs = train[['image']].drop_duplicates()\nunq_valid_imgs = valid[['image']].drop_duplicates()\nprint(len(unq_train_imgs), len(unq_valid_imgs))","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.598190Z","iopub.execute_input":"2022-02-26T18:47:40.599232Z","iopub.status.idle":"2022-02-26T18:47:40.622578Z","shell.execute_reply.started":"2022-02-26T18:47:40.599197Z","shell.execute_reply":"2022-02-26T18:47:40.621075Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.623821Z","iopub.execute_input":"2022-02-26T18:47:40.624169Z","iopub.status.idle":"2022-02-26T18:47:40.695309Z","shell.execute_reply.started":"2022-02-26T18:47:40.624134Z","shell.execute_reply":"2022-02-26T18:47:40.694167Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class extractImageFeatureResNetDataSet():\n    def __init__(self, data):\n        self.data = data \n        self.scaler = transforms.Resize([224, 224])\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n        self.to_tensor = transforms.ToTensor()\n    def __len__(self):  \n        return len(self.data)\n\n    def __getitem__(self, idx):\n\n        image_name = self.data.iloc[idx]['image']\n        img_loc = '../input/flickr8k/Images/'+str(image_name)\n\n        img = Image.open(img_loc)\n        t_img = self.normalize(self.to_tensor(self.scaler(img)))\n\n        return image_name, t_img","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.696840Z","iopub.execute_input":"2022-02-26T18:47:40.697324Z","iopub.status.idle":"2022-02-26T18:47:40.709329Z","shell.execute_reply.started":"2022-02-26T18:47:40.697181Z","shell.execute_reply":"2022-02-26T18:47:40.708513Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_train_imgs)\ntrain_ImageDataloader_ResNet = DataLoader(train_ImageDataset_ResNet, batch_size = 1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.713219Z","iopub.execute_input":"2022-02-26T18:47:40.715794Z","iopub.status.idle":"2022-02-26T18:47:40.721813Z","shell.execute_reply.started":"2022-02-26T18:47:40.714071Z","shell.execute_reply":"2022-02-26T18:47:40.720988Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"valid_ImageDataset_ResNet = extractImageFeatureResNetDataSet(unq_valid_imgs)\nvalid_ImageDataloader_ResNet = DataLoader(valid_ImageDataset_ResNet, batch_size = 1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.725341Z","iopub.execute_input":"2022-02-26T18:47:40.726900Z","iopub.status.idle":"2022-02-26T18:47:40.735887Z","shell.execute_reply.started":"2022-02-26T18:47:40.726862Z","shell.execute_reply":"2022-02-26T18:47:40.734859Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"resnet18 = torchvision.models.resnet18(pretrained=True).to(device)\nresnet18.eval()\nlist(resnet18._modules)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:40.737466Z","iopub.execute_input":"2022-02-26T18:47:40.737831Z","iopub.status.idle":"2022-02-26T18:47:48.211773Z","shell.execute_reply.started":"2022-02-26T18:47:40.737780Z","shell.execute_reply":"2022-02-26T18:47:48.210983Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"resNet18Layer4 = resnet18._modules.get('layer4').to(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:48.215814Z","iopub.execute_input":"2022-02-26T18:47:48.216080Z","iopub.status.idle":"2022-02-26T18:47:48.220592Z","shell.execute_reply.started":"2022-02-26T18:47:48.216052Z","shell.execute_reply":"2022-02-26T18:47:48.219784Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def get_vector(t_img):\n    \n    t_img = Variable(t_img)\n    my_embedding = torch.zeros(1, 512, 7, 7)\n    def copy_data(m, i, o):\n        my_embedding.copy_(o.data)\n    \n    h = resNet18Layer4.register_forward_hook(copy_data)\n    resnet18(t_img)\n    \n    h.remove()\n    return my_embedding","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:48.222570Z","iopub.execute_input":"2022-02-26T18:47:48.223219Z","iopub.status.idle":"2022-02-26T18:47:48.234515Z","shell.execute_reply.started":"2022-02-26T18:47:48.223180Z","shell.execute_reply":"2022-02-26T18:47:48.233583Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"extract_imgFtr_ResNet_train = {}\nfor image_name, t_img in tqdm(train_ImageDataloader_ResNet):\n    t_img = t_img.to(device)\n    embdg = get_vector(t_img)\n    \n    extract_imgFtr_ResNet_train[image_name[0]] = embdg\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:47:48.235810Z","iopub.execute_input":"2022-02-26T18:47:48.236377Z","iopub.status.idle":"2022-02-26T18:50:50.112899Z","shell.execute_reply.started":"2022-02-26T18:47:48.236340Z","shell.execute_reply":"2022-02-26T18:50:50.112076Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"a_file = open(\"./EncodedImageTrainResNet.pkl\", \"wb\")\npickle.dump(extract_imgFtr_ResNet_train, a_file)\na_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:50:50.114182Z","iopub.execute_input":"2022-02-26T18:50:50.114691Z","iopub.status.idle":"2022-02-26T18:50:51.737660Z","shell.execute_reply.started":"2022-02-26T18:50:50.114649Z","shell.execute_reply":"2022-02-26T18:50:51.736784Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"extract_imgFtr_ResNet_valid = {}\nfor image_name, t_img in tqdm(valid_ImageDataloader_ResNet):\n    t_img = t_img.to(device)\n    embdg = get_vector(t_img)\n \n    extract_imgFtr_ResNet_valid[image_name[0]] = embdg","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:50:51.739016Z","iopub.execute_input":"2022-02-26T18:50:51.739406Z","iopub.status.idle":"2022-02-26T18:51:13.228435Z","shell.execute_reply.started":"2022-02-26T18:50:51.739367Z","shell.execute_reply":"2022-02-26T18:51:13.227615Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"a_file = open(\"./EncodedImageValidResNet.pkl\", \"wb\")\npickle.dump(extract_imgFtr_ResNet_valid, a_file)\na_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:51:13.229728Z","iopub.execute_input":"2022-02-26T18:51:13.230351Z","iopub.status.idle":"2022-02-26T18:51:13.406636Z","shell.execute_reply.started":"2022-02-26T18:51:13.230307Z","shell.execute_reply":"2022-02-26T18:51:13.405769Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Create DataLoader which will be used to load data into Transformer Model.\n## FlickerDataSetResnet will return caption sequence, 1 timestep left shifted caption sequence which model will predict and Stored Image features from ResNet.","metadata":{}},{"cell_type":"code","source":"class FlickerDataSetResnet():\n    def __init__(self, data, pkl_file):\n        self.data = data\n        self.encodedImgs = pd.read_pickle(pkl_file)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n    \n        caption_seq = self.data.iloc[idx]['text_seq']\n        target_seq = caption_seq[1:]+[0]\n\n        image_name = self.data.iloc[idx]['image']\n        image_tensor = self.encodedImgs[image_name]\n        image_tensor = image_tensor.permute(0,2,3,1)\n        image_tensor_view = image_tensor.view(image_tensor.size(0), -1, image_tensor.size(3))\n\n        return torch.tensor(caption_seq), torch.tensor(target_seq), image_tensor_view","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:51:13.407952Z","iopub.execute_input":"2022-02-26T18:51:13.408331Z","iopub.status.idle":"2022-02-26T18:51:13.418614Z","shell.execute_reply.started":"2022-02-26T18:51:13.408293Z","shell.execute_reply":"2022-02-26T18:51:13.417772Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train_dataset_resnet = FlickerDataSetResnet(train, 'EncodedImageTrainResNet.pkl')\ntrain_dataloader_resnet = DataLoader(train_dataset_resnet, batch_size = 32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:51:13.419962Z","iopub.execute_input":"2022-02-26T18:51:13.420368Z","iopub.status.idle":"2022-02-26T18:51:15.015793Z","shell.execute_reply.started":"2022-02-26T18:51:13.420332Z","shell.execute_reply":"2022-02-26T18:51:15.014907Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"valid_dataset_resnet = FlickerDataSetResnet(valid, 'EncodedImageValidResNet.pkl')\nvalid_dataloader_resnet = DataLoader(valid_dataset_resnet, batch_size = 32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:51:15.017188Z","iopub.execute_input":"2022-02-26T18:51:15.017515Z","iopub.status.idle":"2022-02-26T18:51:15.114630Z","shell.execute_reply.started":"2022-02-26T18:51:15.017480Z","shell.execute_reply":"2022-02-26T18:51:15.113688Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Create Transformer Decoder Model. This Model will take caption sequence and the extracted resnet image features as input and ouput 1 timestep shifted (left) caption sequence. \n## In the Transformer decoder, lookAhead and padding mask has also been applied","metadata":{}},{"cell_type":"markdown","source":"### Position Embedding","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=max_seq_len):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n\n    def forward(self, x):\n        if self.pe.size(0) < x.size(0):\n            self.pe = self.pe.repeat(x.size(0), 1, 1).to(device)\n        self.pe = self.pe[:x.size(0), : , : ]\n        \n        x = x + self.pe\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:51:15.115975Z","iopub.execute_input":"2022-02-26T18:51:15.116339Z","iopub.status.idle":"2022-02-26T18:51:15.127737Z","shell.execute_reply.started":"2022-02-26T18:51:15.116303Z","shell.execute_reply":"2022-02-26T18:51:15.126688Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## Transformer Decoder","metadata":{}},{"cell_type":"code","source":"class ImageCaptionModel(nn.Module):\n    def __init__(self, n_head, n_decoder_layer, vocab_size, embedding_size):\n        super(ImageCaptionModel, self).__init__()\n        self.pos_encoder = PositionalEncoding(embedding_size, 0.1)\n        self.TransformerDecoderLayer = nn.TransformerDecoderLayer(d_model =  embedding_size, nhead = n_head)\n        self.TransformerDecoder = nn.TransformerDecoder(decoder_layer = self.TransformerDecoderLayer, num_layers = n_decoder_layer)\n        self.embedding_size = embedding_size\n        self.embedding = nn.Embedding(vocab_size , embedding_size)\n        self.last_linear_layer = nn.Linear(embedding_size, vocab_size)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.last_linear_layer.bias.data.zero_()\n        self.last_linear_layer.weight.data.uniform_(-initrange, initrange)\n\n    def generate_Mask(self, size, decoder_inp):\n        decoder_input_mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n        decoder_input_mask = decoder_input_mask.float().masked_fill(decoder_input_mask == 0, float('-inf')).masked_fill(decoder_input_mask == 1, float(0.0))\n\n        decoder_input_pad_mask = decoder_inp.float().masked_fill(decoder_inp == 0, float(0.0)).masked_fill(decoder_inp > 0, float(1.0))\n        decoder_input_pad_mask_bool = decoder_inp == 0\n\n        return decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool\n\n    def forward(self, encoded_image, decoder_inp):\n        encoded_image = encoded_image.permute(1,0,2)\n        \n\n        decoder_inp_embed = self.embedding(decoder_inp)* math.sqrt(self.embedding_size)\n        \n        decoder_inp_embed = self.pos_encoder(decoder_inp_embed)\n        decoder_inp_embed = decoder_inp_embed.permute(1,0,2)\n        \n\n        decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool = self.generate_Mask(decoder_inp.size(1), decoder_inp)\n        decoder_input_mask = decoder_input_mask.to(device)\n        decoder_input_pad_mask = decoder_input_pad_mask.to(device)\n        decoder_input_pad_mask_bool = decoder_input_pad_mask_bool.to(device)\n        \n\n        decoder_output = self.TransformerDecoder(tgt = decoder_inp_embed, memory = encoded_image, tgt_mask = decoder_input_mask, tgt_key_padding_mask = decoder_input_pad_mask_bool)\n        \n        final_output = self.last_linear_layer(decoder_output)\n\n        return final_output,  decoder_input_pad_mask\n","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:51:15.129227Z","iopub.execute_input":"2022-02-26T18:51:15.129576Z","iopub.status.idle":"2022-02-26T18:51:15.142234Z","shell.execute_reply.started":"2022-02-26T18:51:15.129537Z","shell.execute_reply":"2022-02-26T18:51:15.141395Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Train the Model","metadata":{}},{"cell_type":"markdown","source":"### The cross entropy loss has been masked at time steps where input token is <'pad'>.","metadata":{}},{"cell_type":"code","source":"EPOCH = 30","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:51:15.143522Z","iopub.execute_input":"2022-02-26T18:51:15.144063Z","iopub.status.idle":"2022-02-26T18:51:15.152988Z","shell.execute_reply.started":"2022-02-26T18:51:15.144024Z","shell.execute_reply":"2022-02-26T18:51:15.151994Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"ictModel = ImageCaptionModel(16, 4, vocab_size, 512).to(device)\noptimizer = torch.optim.Adam(ictModel.parameters(), lr = 0.00001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience=2, verbose = True)\ncriterion = torch.nn.CrossEntropyLoss(reduction='none')\nmin_val_loss = np.float('Inf')","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:51:15.154614Z","iopub.execute_input":"2022-02-26T18:51:15.154997Z","iopub.status.idle":"2022-02-26T18:51:15.435390Z","shell.execute_reply.started":"2022-02-26T18:51:15.154941Z","shell.execute_reply":"2022-02-26T18:51:15.434471Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"for epoch in tqdm(range(EPOCH)):\n    total_epoch_train_loss = 0\n    total_epoch_valid_loss = 0\n    total_train_words = 0\n    total_valid_words = 0\n    ictModel.train()\n\n    ### Train Loop\n    for caption_seq, target_seq, image_embed in train_dataloader_resnet:\n\n        optimizer.zero_grad()\n\n        image_embed = image_embed.squeeze(1).to(device)\n        caption_seq = caption_seq.to(device)\n        target_seq = target_seq.to(device)\n\n        output, padding_mask = ictModel.forward(image_embed, caption_seq)\n        output = output.permute(1, 2, 0)\n\n        loss = criterion(output,target_seq)\n\n        loss_masked = torch.mul(loss, padding_mask)\n\n        final_batch_loss = torch.sum(loss_masked)/torch.sum(padding_mask)\n\n        final_batch_loss.backward()\n        optimizer.step()\n        total_epoch_train_loss += torch.sum(loss_masked).detach().item()\n        total_train_words += torch.sum(padding_mask)\n\n \n    total_epoch_train_loss = total_epoch_train_loss/total_train_words\n  \n\n    ### Eval Loop\n    ictModel.eval()\n    with torch.no_grad():\n        for caption_seq, target_seq, image_embed in valid_dataloader_resnet:\n\n            image_embed = image_embed.squeeze(1).to(device)\n            caption_seq = caption_seq.to(device)\n            target_seq = target_seq.to(device)\n\n            output, padding_mask = ictModel.forward(image_embed, caption_seq)\n            output = output.permute(1, 2, 0)\n\n            loss = criterion(output,target_seq)\n\n            loss_masked = torch.mul(loss, padding_mask)\n\n            total_epoch_valid_loss += torch.sum(loss_masked).detach().item()\n            total_valid_words += torch.sum(padding_mask)\n\n    total_epoch_valid_loss = total_epoch_valid_loss/total_valid_words\n  \n    print(\"Epoch -> \", epoch,\" Training Loss -> \", total_epoch_train_loss.item(), \"Eval Loss -> \", total_epoch_valid_loss.item() )\n  \n    if min_val_loss > total_epoch_valid_loss:\n        print(\"Writing Model at epoch \", epoch)\n        torch.save(ictModel, './BestModel')\n        min_val_loss = total_epoch_valid_loss\n  \n\n    scheduler.step(total_epoch_valid_loss.item())\n","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:51:15.437696Z","iopub.execute_input":"2022-02-26T18:51:15.438050Z","iopub.status.idle":"2022-02-26T19:26:30.430394Z","shell.execute_reply.started":"2022-02-26T18:51:15.438013Z","shell.execute_reply":"2022-02-26T19:26:30.429501Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets Generate Captions !!!","metadata":{}},{"cell_type":"code","source":"model = torch.load('./BestModel')\nstart_token = word_to_index['<start>']\nend_token = word_to_index['<end>']\npad_token = word_to_index['<pad>']\nmax_seq_len = 33\nprint(start_token, end_token, pad_token)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:26:30.431915Z","iopub.execute_input":"2022-02-26T19:26:30.432539Z","iopub.status.idle":"2022-02-26T19:26:30.564557Z","shell.execute_reply.started":"2022-02-26T19:26:30.432495Z","shell.execute_reply":"2022-02-26T19:26:30.563387Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"valid_img_embed = pd.read_pickle('EncodedImageValidResNet.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:26:30.566182Z","iopub.execute_input":"2022-02-26T19:26:30.566583Z","iopub.status.idle":"2022-02-26T19:26:30.672973Z","shell.execute_reply.started":"2022-02-26T19:26:30.566542Z","shell.execute_reply":"2022-02-26T19:26:30.672128Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### Here in the below function,we are generating caption in beam search. K defines the topK token to look at each time step","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport requests\nfrom io import BytesIO\n\nresponse = requests.get(\"https://newevolutiondesigns.com/images/freebies/dog-wallpaper-11.jpg\")\nimGG = Image.open(BytesIO(response.content))","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:56:56.257964Z","iopub.execute_input":"2022-02-26T19:56:56.258336Z","iopub.status.idle":"2022-02-26T19:56:56.793083Z","shell.execute_reply.started":"2022-02-26T19:56:56.258305Z","shell.execute_reply":"2022-02-26T19:56:56.792247Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"def generate_caption(K, img_nm): \n    img_loc = '../input/flickr8k/Images/'+str(img_nm)\n    image = Image.open(img_loc).convert(\"RGB\")\n    #image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n    plt.imshow(image)\n\n    model.eval() \n    valid_img_df = valid[valid['image']==img_nm]\n    print(\"Actual Caption : \")\n    print(valid_img_df['caption'].tolist())\n    img_embed = valid_img_embed[img_nm].to(device)\n\n\n    img_embed = img_embed.permute(0,2,3,1)\n    img_embed = img_embed.view(img_embed.size(0), -1, img_embed.size(3))\n\n\n    input_seq = [pad_token]*max_seq_len\n    input_seq[0] = start_token\n\n    input_seq = torch.tensor(input_seq).unsqueeze(0).to(device)\n    predicted_sentence = []\n    with torch.no_grad():\n        for eval_iter in range(0, max_seq_len):\n\n            output, padding_mask = model.forward(img_embed, input_seq)\n\n            output = output[eval_iter, 0, :]\n\n            values = torch.topk(output, K).values.tolist()\n            indices = torch.topk(output, K).indices.tolist()\n\n            next_word_index = random.choices(indices, values, k = 1)[0]\n\n            next_word = index_to_word[next_word_index]\n\n            input_seq[:, eval_iter+1] = next_word_index\n\n\n            if next_word == '<end>' :\n                break\n\n            predicted_sentence.append(next_word)\n    print(\"\\n\")\n    print(\"Predicted caption : \")\n    print(\" \".join(predicted_sentence+['.']))","metadata":{"execution":{"iopub.status.busy":"2022-02-26T20:00:34.093990Z","iopub.execute_input":"2022-02-26T20:00:34.094359Z","iopub.status.idle":"2022-02-26T20:00:34.104411Z","shell.execute_reply.started":"2022-02-26T20:00:34.094327Z","shell.execute_reply":"2022-02-26T20:00:34.103034Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"### 1st Example ","metadata":{}},{"cell_type":"code","source":"generate_caption(1, unq_valid_imgs.iloc[50]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T20:00:37.746624Z","iopub.execute_input":"2022-02-26T20:00:37.746940Z","iopub.status.idle":"2022-02-26T20:00:37.996401Z","shell.execute_reply.started":"2022-02-26T20:00:37.746909Z","shell.execute_reply":"2022-02-26T20:00:37.995444Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"generate_caption(2, unq_valid_imgs.iloc[50]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:26:30.985332Z","iopub.execute_input":"2022-02-26T19:26:30.985650Z","iopub.status.idle":"2022-02-26T19:26:31.215615Z","shell.execute_reply.started":"2022-02-26T19:26:30.985616Z","shell.execute_reply":"2022-02-26T19:26:31.214807Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"### 2nd Example","metadata":{}},{"cell_type":"code","source":"generate_caption(1, unq_valid_imgs.iloc[100]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:26:31.216877Z","iopub.execute_input":"2022-02-26T19:26:31.217388Z","iopub.status.idle":"2022-02-26T19:26:31.442615Z","shell.execute_reply.started":"2022-02-26T19:26:31.217349Z","shell.execute_reply":"2022-02-26T19:26:31.441827Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"path = \"https://newevolutiondesigns.com/images/freebies/dog-wallpaper-11.jpg\"\ngenerate_caption(i, path)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:49:08.509427Z","iopub.execute_input":"2022-02-26T19:49:08.509775Z","iopub.status.idle":"2022-02-26T19:49:08.536440Z","shell.execute_reply.started":"2022-02-26T19:49:08.509745Z","shell.execute_reply":"2022-02-26T19:49:08.535115Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"for i in range(1, 8):\n    generate_caption(i, unq_valid_imgs.iloc[100]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:46:30.790030Z","iopub.execute_input":"2022-02-26T19:46:30.790393Z","iopub.status.idle":"2022-02-26T19:46:31.879851Z","shell.execute_reply.started":"2022-02-26T19:46:30.790361Z","shell.execute_reply":"2022-02-26T19:46:31.878879Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"generate_caption(3, unq_valid_imgs.iloc[100]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:44:59.559722Z","iopub.execute_input":"2022-02-26T19:44:59.560058Z","iopub.status.idle":"2022-02-26T19:44:59.910393Z","shell.execute_reply.started":"2022-02-26T19:44:59.560025Z","shell.execute_reply":"2022-02-26T19:44:59.904186Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"### 3rd Example","metadata":{}},{"cell_type":"code","source":"generate_caption(1, unq_valid_imgs.iloc[500]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:26:31.693045Z","iopub.execute_input":"2022-02-26T19:26:31.693642Z","iopub.status.idle":"2022-02-26T19:26:31.924414Z","shell.execute_reply.started":"2022-02-26T19:26:31.693600Z","shell.execute_reply":"2022-02-26T19:26:31.923425Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"generate_caption(2, unq_valid_imgs.iloc[500]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:26:31.925868Z","iopub.execute_input":"2022-02-26T19:26:31.926488Z","iopub.status.idle":"2022-02-26T19:26:32.154667Z","shell.execute_reply.started":"2022-02-26T19:26:31.926447Z","shell.execute_reply":"2022-02-26T19:26:32.153801Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"### 4rth Example","metadata":{}},{"cell_type":"code","source":"generate_caption(1, unq_valid_imgs.iloc[601]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:43:53.933444Z","iopub.execute_input":"2022-02-26T19:43:53.933766Z","iopub.status.idle":"2022-02-26T19:43:54.154066Z","shell.execute_reply.started":"2022-02-26T19:43:53.933735Z","shell.execute_reply":"2022-02-26T19:43:54.153262Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"generate_caption(2, unq_valid_imgs.iloc[600]['image'])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:26:32.394879Z","iopub.execute_input":"2022-02-26T19:26:32.395519Z","iopub.status.idle":"2022-02-26T19:26:32.789211Z","shell.execute_reply.started":"2022-02-26T19:26:32.395469Z","shell.execute_reply":"2022-02-26T19:26:32.788206Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## Thanks for going through the whole work. Please do upvote the notebook if you liked it.","metadata":{}},{"cell_type":"code","source":"# Model class must be defined somewhere\nPATH = \"state_dict_model.pt\"\n\ntorch.save(model, PATH)\nmodel = torch.load(PATH)\nmodel.eval()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:40:30.381842Z","iopub.execute_input":"2022-02-26T19:40:30.382200Z","iopub.status.idle":"2022-02-26T19:40:31.114635Z","shell.execute_reply.started":"2022-02-26T19:40:30.382167Z","shell.execute_reply":"2022-02-26T19:40:31.113767Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"model_scripted = torch.jit.script(model) # Export to TorchScript\n","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:42:25.850050Z","iopub.execute_input":"2022-02-26T19:42:25.850458Z","iopub.status.idle":"2022-02-26T19:42:25.909654Z","shell.execute_reply.started":"2022-02-26T19:42:25.850426Z","shell.execute_reply":"2022-02-26T19:42:25.907715Z"},"trusted":true},"execution_count":58,"outputs":[]}]}